# backend/services/rag.py
import time
from typing import List
from langchain.prompts import PromptTemplate
from langchain_openai.chat_models import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.llms.ollama import Ollama
from langchain_community.vectorstores import Chroma

from backend.core.config import settings
from backend.services.retrievers import get_retriever
from backend.services.safety import is_safe
from backend.services.embeddings import get_embeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
# =======================
# ç³»ç»Ÿæç¤ºè¯
# =======================
SYSTEM_PROMPT = """You are a professional technical assistant for Link ECU, specializing in engine control unit (ECU) systems.
Your task:
- Answer user questions **only** based on the provided Link ECU manuals and documents.
- **Do not make up information.** If the answer is not explicitly covered in the context, respond exactly with:
  "I'm sorry, I couldn't find this in the current documentation."
- Be **clear, concise, and structured**. 
- When describing procedures, break them down into **step-by-step instructions** using numbered lists.
- Always highlight **safety warnings** or potential hazards at the beginning of your response if relevant.
- Keep the tone **professional and factual**, avoiding speculation or casual language.

Context for answering will be provided in the `Context:` section below. 
Only use this context â€” do not rely on prior knowledge or assumptions.
"""

QA_PROMPT = PromptTemplate(
    template=SYSTEM_PROMPT + "\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:",
    input_variables=["context", "question"],
)

# =======================
# å¯åŠ¨æ—¶åˆå§‹åŒ–
# =======================
print("ğŸ”¹ [Startup] Initializing Embeddings and Chroma VectorDB ...")

# 1. åŠ è½½ BGE Embeddings
EMBEDDINGS = get_embeddings(device="mps")  # Mac M1/M2/M3 ä½¿ç”¨ 'mps'

# 2. åŠ è½½ Chroma å‘é‡åº“
VECTORDB = Chroma(
    collection_name="link_ecu_docs",
    embedding_function=EMBEDDINGS,
    persist_directory=settings.vector_dir,
)

# 3. æ„å»º Retriever
RETRIEVER = VECTORDB.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.5},
)

print("âœ… [Startup] VectorDB and Embeddings initialized successfully!")

# =======================
# æ ¸å¿ƒé—®ç­”é€»è¾‘ï¼ˆé¿å…é‡å¤æ£€ç´¢ï¼‰
# =======================
def answer_question(question: str, model: str = "gpt-5-mini", provider: str = "openai"):
    """
    1. æ£€ç´¢ä¸€æ¬¡æ–‡æ¡£
    2. æ‰‹åŠ¨æ‹¼æ¥ä¸Šä¸‹æ–‡
    3. ä¼ ç»™ LLM ç”Ÿæˆç­”æ¡ˆ
    """
    total_start = time.perf_counter()
    print(f"\nğŸ•’ Start processing question: {question}")

    # ---------- 1. å®‰å…¨æ£€æŸ¥ ----------
    t0 = time.perf_counter()
    safe, msg = is_safe(question)
    t1 = time.perf_counter()
    print(f"å®‰å…¨æ£€æŸ¥è€—æ—¶: {(t1 - t0) * 1000:.2f} ms")

    if not safe:
        return "âš ï¸ The request is blocked due to safety policy.", []

    # ---------- 2. åˆå§‹åŒ– LLM ----------
    t2 = time.perf_counter()
    llm = get_llm_by_provider(provider, model, settings.temperature,  streaming=True)
    t3 = time.perf_counter()
    print(f"åŠ è½½ LLM è€—æ—¶: {(t3 - t2) * 1000:.2f} ms")

    # ---------- 3. **ä»…æ£€ç´¢ä¸€æ¬¡æ–‡æ¡£** ----------
    t4 = time.perf_counter()
    docs = RETRIEVER.get_relevant_documents(question)  # âœ… æ›¿æ¢ invokeï¼Œè¿”å› Document å¯¹è±¡åˆ—è¡¨
    t5 = time.perf_counter()
    print(f"æ£€ç´¢è€—æ—¶: {(t5 - t4) * 1000:.2f} ms")

    # ---------- 4. æ‰‹åŠ¨æ‹¼æ¥ä¸Šä¸‹æ–‡ ----------
    MAX_DOC_LEN = 1200  # é™åˆ¶æ¯æ®µæ–‡æ¡£é•¿åº¦ï¼Œé¿å…è¶…é•¿ Prompt
    context = "\n\n".join([d.page_content[:MAX_DOC_LEN] for d in docs])

    # æ„é€ æœ€ç»ˆ Prompt
    final_prompt = QA_PROMPT.format(context=context, question=question)

    # ---------- 5. è°ƒç”¨ LLM ----------
    t6 = time.perf_counter()
    response = llm.invoke(final_prompt)  # è¿”å› AIMessage å¯¹è±¡
    answer = response.content.strip()  # å– content ä¸­çš„å­—ç¬¦ä¸²
    t7 = time.perf_counter()
    print(f"è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆè€—æ—¶: {(t7 - t6):.2f} ç§’")

    # ---------- 6. æ•´ç†å¼•ç”¨ä¿¡æ¯ ----------
    sources = []
    for d in docs:
        meta = d.metadata or {}
        sources.append({
            "source": meta.get("source", "unknown"),
            "page": meta.get("page"),
            "snippet": d.page_content[:180]
        })

    total_time = t7 - total_start
    print(f"ğŸ”¹ æ€»è€—æ—¶: {total_time:.2f} ç§’\n")

    return answer, sources, {
        "retrieval_time": round((t5 - t4) * 1000, 2),
        "llm_time": round((t7 - t6) * 1000, 2)
    }

# =======================
# å¤šæ¨¡å‹æ”¯æŒ
# =======================
def get_llm_by_provider(provider: str, model: str, temperature: float = 0.0,streaming: bool = False):
    """
    æ ¹æ® provider å‚æ•°é€‰æ‹©å¯¹åº”çš„ LLM
    """
    if provider == "openai":
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            streaming=streaming,
            callbacks=[StreamingStdOutCallbackHandler()],
            openai_api_key=settings.openai_api_key
        )
    elif provider == "claude":
        return ChatAnthropic(
            model=model,
            temperature=temperature,
            anthropic_api_key=settings.anthropic_api_key
        )
    elif provider == "local":
        # ä½¿ç”¨ Ollama è°ƒç”¨æœ¬åœ° LLM
        return Ollama(model=model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")
